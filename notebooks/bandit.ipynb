{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from gymnax.environments import spaces\n",
    "from typing import Optional\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from experior.utils import PRNGSequence\n",
    "from experior.envs import BayesStochasticBandit\n",
    "from experior.bandit_agents import (\n",
    "  make_thompson_sampling,\n",
    "  make_max_ent_thompson_sampling,\n",
    "  make_bernoulli_thompson_sampling,\n",
    "  make_multi_armed_explore_ucb,\n",
    "  make_multi_armed_ucb,\n",
    "  make_multi_armed_bc,\n",
    "  LinearDiscreteRewardModel\n",
    ")\n",
    "from experior.experts import generate_optimal_trajectories\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune the hyperparameters for all baselines by optimizing the Bayesian regret for a beta task distribution $\\mu^\\star = Beta(0.5, 0.5)$ with $K = 10$ and $T = 1024$. We sample $N_{\\text{task}} = 100$ to estimate the Bayesian regret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 5\n",
    "NUM_ENVS = 100\n",
    "NUM_STEPS = 1024\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-TS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search over the following hyperparameters:\n",
    "\n",
    "- SGLD batch size in `[128, 256, 1024]`\n",
    "- Number of Langevin update steps per episode `[1, 5, 10, 20]`\n",
    "- SGLD learning rate `[1e-1, 1e-2, 5e-2, 1e-3]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:23:07 INFO     Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Interpreter\n",
      "2024-04-09 15:23:07 INFO     Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2024-04-09 15:23:15 INFO     batch size: 128, updates per step: 1 done!\n",
      "2024-04-09 15:23:20 INFO     batch size: 128, updates per step: 5 done!\n",
      "2024-04-09 15:23:22 INFO     batch size: 128, updates per step: 10 done!\n",
      "2024-04-09 15:23:25 INFO     batch size: 128, updates per step: 20 done!\n",
      "2024-04-09 15:23:28 INFO     batch size: 256, updates per step: 1 done!\n",
      "2024-04-09 15:23:30 INFO     batch size: 256, updates per step: 5 done!\n",
      "2024-04-09 15:23:33 INFO     batch size: 256, updates per step: 10 done!\n",
      "2024-04-09 15:23:36 INFO     batch size: 256, updates per step: 20 done!\n",
      "2024-04-09 15:23:38 INFO     batch size: 1024, updates per step: 1 done!\n",
      "2024-04-09 15:23:41 INFO     batch size: 1024, updates per step: 5 done!\n",
      "2024-04-09 15:23:44 INFO     batch size: 1024, updates per step: 10 done!\n",
      "2024-04-09 15:23:48 INFO     batch size: 1024, updates per step: 20 done!\n"
     ]
    }
   ],
   "source": [
    "# bernoulli bandit setup\n",
    "alpha, beta = 0.5, 0.5\n",
    "action_space = spaces.Discrete(NUM_ACTIONS)\n",
    "prior_function = jax.tree_util.Partial(\n",
    "    lambda key, _: jax.random.beta(\n",
    "        key,\n",
    "        alpha * jnp.ones((NUM_ACTIONS,)),\n",
    "        beta * jnp.ones((NUM_ACTIONS,)),\n",
    "        shape=(NUM_ACTIONS,),\n",
    "    )\n",
    ")\n",
    "reward_dist_fn = jax.tree_util.Partial(\n",
    "    lambda key, means, _, action: jax.random.bernoulli(key, means[action]).astype(\n",
    "        jnp.float32\n",
    "    )\n",
    ")\n",
    "best_action_value_fn = jax.tree_util.Partial(\n",
    "    lambda means, _: (means.argmax(), means.max())\n",
    ")\n",
    "reward_mean_fn = jax.tree_util.Partial(lambda means, _, action: means[action])\n",
    "mutli_armed_bandit = BayesStochasticBandit(\n",
    "    action_space, prior_function, reward_dist_fn, reward_mean_fn, best_action_value_fn\n",
    ")\n",
    "\n",
    "feature_fn = jax.tree_util.Partial(\n",
    "    lambda obs, action: jax.nn.one_hot(action, NUM_ACTIONS)\n",
    ")\n",
    "\n",
    "reward_model = LinearDiscreteRewardModel(\n",
    "    n_actions=NUM_ACTIONS,\n",
    "    params_dim=NUM_ACTIONS,\n",
    "    feature_fn=feature_fn,\n",
    "    dist=\"bernoulli\",\n",
    ")\n",
    "\n",
    "# search over SGLD batch size, updates per step, and learning rate,\n",
    "b_sizes = [128, 256, 1024]\n",
    "steps = [1, 5, 10, 20]\n",
    "langevin_learning_rates = jnp.array([1e-1, 1e-2, 5e-2, 1e-3])\n",
    "results = jnp.zeros((len(b_sizes), len(steps), langevin_learning_rates.shape[0]))\n",
    "\n",
    "for i, b_size in enumerate(b_sizes):\n",
    "    for j, update_per_step in enumerate(steps):\n",
    "        ts_train = make_thompson_sampling(\n",
    "            env=mutli_armed_bandit,\n",
    "            reward_model=reward_model,\n",
    "            num_envs=NUM_ENVS,\n",
    "            total_steps=NUM_STEPS,\n",
    "            langevin_batch_size=b_size,\n",
    "            langevin_updates_per_step=update_per_step,\n",
    "        )\n",
    "\n",
    "        jit_train_ts_hyper = jax.vmap(jax.jit(ts_train), in_axes=(None, 0))\n",
    "\n",
    "        state, hyper_metrics = jit_train_ts_hyper(\n",
    "            jax.random.PRNGKey(SEED), langevin_learning_rates\n",
    "        )  # shape: (n_lrs, n_steps, n_envs, ...)\n",
    "        bayes_regret = (\n",
    "            (hyper_metrics[\"optimal_value\"] - hyper_metrics[\"reward_mean\"])\n",
    "            .mean(axis=-1)\n",
    "            .sum(axis=-1)\n",
    "        )  # shape: (n_lrs,)\n",
    "        results = results.at[i, j, :].set(bayes_regret)\n",
    "        logging.info(f\"batch size: {b_size}, updates per step: {update_per_step} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:23:49 INFO     \n",
      "    Best hyperparameters: batch: 1024,\n",
      "    langevin steps: 5,\n",
      "    learning rate: 0.05000000074505806,\n",
      "    regret: 18.292362213134766\n"
     ]
    }
   ],
   "source": [
    "indices = jnp.where(results == results.min())\n",
    "logging.info(\n",
    "    f\"\"\"\n",
    "    Best hyperparameters: batch: {b_sizes[indices[0][0]]},\n",
    "    langevin steps: {steps[indices[1][0]]},\n",
    "    learning rate: {langevin_learning_rates[indices[2][0]]},\n",
    "    regret: {results.min()}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExPerior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ExPerior, we search over the following hyperparameters:\n",
    "\n",
    "- SGLD learning rate `[1e-2, 5e-2, 1e-3, 2.5e-4]`\n",
    "- Lagrange multiplier ($\\lambda^\\star$) `[0.1, 1.0, 10.0, 50.0, 100.0]`\n",
    "- Expert competence level ($\\beta$) `[0.1, 1.0, 3.0, 10.0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fix other hyperparameters as\n",
    "\n",
    "B_SIZE = 1024\n",
    "UPDATES_PER_STEP = 5\n",
    "\n",
    "MAX_ENT_SEED = 712\n",
    "MAX_ENT_LR = 1e-2  # learning rate to optimize (6) in Proposition 1\n",
    "MAX_ENT_STEPS = 1000  # number of steps to optimize (6) in Proposition 1\n",
    "MAX_ENT_SAMPLES = (\n",
    "    1024  # number of samples to estimate the expectation in (6) in Proposition 1\n",
    ")\n",
    "\n",
    "EXPERT_TRAJ_SEED = 1233\n",
    "N_EXPERT_TRAJECTORY = 1000  # number of expert trajectories `N`` in Proposition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate expert trajectories\n",
    "expert_trajectories = generate_optimal_trajectories(\n",
    "    jax.random.PRNGKey(EXPERT_TRAJ_SEED),\n",
    "    mutli_armed_bandit,\n",
    "    N_EXPERT_TRAJECTORY,\n",
    "    1,\n",
    "    None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = LinearDiscreteRewardModel(\n",
    "    n_actions=NUM_ACTIONS,\n",
    "    params_dim=NUM_ACTIONS,\n",
    "    feature_fn=feature_fn,\n",
    "    dist=\"bernoulli\",\n",
    ")\n",
    "\n",
    "max_ent_ts_train = make_max_ent_thompson_sampling(\n",
    "    env=mutli_armed_bandit,\n",
    "    reward_model=reward_model,\n",
    "    num_envs=NUM_ENVS,\n",
    "    total_steps=NUM_STEPS,\n",
    "    langevin_batch_size=B_SIZE,\n",
    "    langevin_updates_per_step=UPDATES_PER_STEP,\n",
    "    max_ent_prior_n_samples=MAX_ENT_SAMPLES,\n",
    "    max_ent_steps=MAX_ENT_STEPS,\n",
    ")\n",
    "\n",
    "\n",
    "langevin_learning_rates = jnp.array([1e-2, 5e-2, 1e-3, 2.5e-4])\n",
    "max_ent_lambdas = jnp.array([0.1, 1.0, 10.0, 50.0, 100.0])\n",
    "expert_betas = jnp.array([0.1, 1.0, 3.0, 10.0])\n",
    "\n",
    "jit_max_ent_train_ts_hyper = jax.vmap(\n",
    "    jax.vmap(\n",
    "        jax.vmap(\n",
    "            jax.jit(max_ent_ts_train),\n",
    "            in_axes=(None, None, None, None, None, None, 0),\n",
    "        ),\n",
    "        in_axes=(None, None, None, None, None, 0, None),\n",
    "    ),\n",
    "    in_axes=(None, None, None, 0, None, None, None),\n",
    ")\n",
    "\n",
    "state, max_ent_state, hyper_metrics = jit_max_ent_train_ts_hyper(\n",
    "    jax.random.PRNGKey(MAX_ENT_SEED),\n",
    "    jax.random.PRNGKey(SEED),\n",
    "    expert_trajectories,\n",
    "    max_ent_lambdas,\n",
    "    MAX_ENT_LR,\n",
    "    expert_betas,\n",
    "    langevin_learning_rates,\n",
    ")  # shape: (n_lambdas, n_betas, n_lrs, n_steps, n_envs ...)\n",
    "bayes_regret = (\n",
    "    (hyper_metrics[\"optimal_value\"] - hyper_metrics[\"reward_mean\"])\n",
    "    .mean(axis=-1)\n",
    "    .sum(axis=-1)\n",
    ")  # shape: (n_lambdas, n_betas, n_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:24:23 INFO     \n",
      "    Best hyperparameters: lambda: 1.0,\n",
      "    beta: 10.0,\n",
      "    learning rate: 0.05000000074505806,\n",
      "    regret: 18.13381576538086\n"
     ]
    }
   ],
   "source": [
    "indices = jnp.where(bayes_regret == bayes_regret.min())\n",
    "logging.info(\n",
    "    f\"\"\"\n",
    "    Best hyperparameters: lambda: {max_ent_lambdas[indices[0][0]]},\n",
    "    beta: {expert_betas[indices[1][0]]},\n",
    "    learning rate: {langevin_learning_rates[indices[2][0]]},\n",
    "    regret: {bayes_regret.min()}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-UCB and UCB-ExPLORe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Naive-UCB, we only search over a constant factor `rho` $\\in$ `[1, 2, 4, 8]` to scale the confidene interval.\n",
    "\n",
    "For UCB-ExPLORe, we also have a `burn_in` $\\in$ `[0, 5, 10, 20, 50, 100]` parameter, which is the number of steps of running Naive-UCB before applying the optimistic rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:24:44 INFO     UCB rho: 1, burn in: 50\n"
     ]
    }
   ],
   "source": [
    "rhos = jnp.array([1, 2, 4, 8])\n",
    "rng = jax.random.PRNGKey(SEED)\n",
    "\n",
    "ucb_train = make_multi_armed_ucb(\n",
    "    env=mutli_armed_bandit, num_envs=NUM_ENVS, total_steps=NUM_STEPS\n",
    ")\n",
    "state, hyper_metrics = jax.vmap(jax.jit(ucb_train), in_axes=(None, 0))(rng, rhos)\n",
    "bayes_regret = (\n",
    "    (hyper_metrics[\"optimal_value\"] - hyper_metrics[\"reward_mean\"])\n",
    "    .mean(axis=-1)\n",
    "    .sum(axis=-1)\n",
    ")  # shape: (n_lrs,)\n",
    "\n",
    "UCB_RHO = rhos[bayes_regret.argmin()]\n",
    "\n",
    "# ucb explore\n",
    "expert_fractions = (\n",
    "    jnp.bincount(expert_trajectories.action.reshape(-1), length=NUM_ACTIONS)\n",
    "    / N_EXPERT_TRAJECTORY\n",
    ")\n",
    "\n",
    "burn_ins = jnp.array([0, 5, 10, 20, 50, 100])\n",
    "ucb_train = make_multi_armed_explore_ucb(\n",
    "    env=mutli_armed_bandit, num_envs=NUM_ENVS, total_steps=NUM_STEPS\n",
    ")\n",
    "state, hyper_metrics = jax.vmap(jax.jit(ucb_train), in_axes=(None, None, None, 0))(\n",
    "    rng, expert_fractions, UCB_RHO, burn_ins\n",
    ")\n",
    "bayes_regret = (\n",
    "    (hyper_metrics[\"optimal_value\"] - hyper_metrics[\"reward_mean\"])\n",
    "    .mean(axis=-1)\n",
    "    .sum(axis=-1)\n",
    ")  # shape: (n_lrs,)\n",
    "UCB_BURN_IN = burn_ins[bayes_regret.argmin()]\n",
    "\n",
    "logging.info(f\"UCB rho: {UCB_RHO}, burn in: {UCB_BURN_IN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORACLE_TS_SGLD_BSIZE = 1024\n",
    "ORACLE_TS_UPDATES_PER_STEP = 5\n",
    "ORACLE_TS_SGLD_LR = 0.05\n",
    "\n",
    "EXPERIOR_SGLD_BSIZE = 1024\n",
    "EXPERIOR_UPDATES_PER_STEP = 5\n",
    "EXPERIOR_SGLD_LR = 0.05\n",
    "EXPERIOR_LAMBDA = 1.0\n",
    "EXPERIOR_BETA = 10.0\n",
    "\n",
    "MAX_ENT_LR = 1e-2\n",
    "MAX_ENT_STEPS = 1000\n",
    "MAX_ENT_SAMPLES = 1024\n",
    "\n",
    "UCB_BURN_IN = 50\n",
    "UCB_RHO = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the environment\n",
    "NUM_STEPS = 1500\n",
    "NUM_ENVS = 128\n",
    "NUM_PRIORS = 256\n",
    "EXPERT_N_TRAJECTORY = 500\n",
    "HORIZON = 1\n",
    "\n",
    "ENV_SEED = 42\n",
    "MAX_ENT_SEED = 512\n",
    "SETTING_SEED = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Baselines and Empirical Regret Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:25:53 INFO     Naive-TS done for 2 actions!\n",
      "2024-04-09 15:25:57 INFO     Oracle-TS done for 2 actions!\n",
      "2024-04-09 15:27:38 INFO     ExPerior done for 2 actions!\n",
      "2024-04-09 15:27:39 INFO     Naive-UCB done for 2 actions!\n",
      "2024-04-09 15:27:40 INFO     UCB-ExPLORe done for 2 actions!\n",
      "2024-04-09 15:27:41 INFO     BC done for 2 actions!\n",
      "2024-04-09 15:28:30 INFO     Naive-TS done for 3 actions!\n",
      "2024-04-09 15:28:34 INFO     Oracle-TS done for 3 actions!\n",
      "2024-04-09 15:30:05 INFO     ExPerior done for 3 actions!\n",
      "2024-04-09 15:30:07 INFO     Naive-UCB done for 3 actions!\n",
      "2024-04-09 15:30:08 INFO     UCB-ExPLORe done for 3 actions!\n",
      "2024-04-09 15:30:10 INFO     BC done for 3 actions!\n",
      "2024-04-09 15:30:51 INFO     Naive-TS done for 4 actions!\n",
      "2024-04-09 15:30:55 INFO     Oracle-TS done for 4 actions!\n",
      "2024-04-09 15:32:27 INFO     ExPerior done for 4 actions!\n",
      "2024-04-09 15:32:29 INFO     Naive-UCB done for 4 actions!\n",
      "2024-04-09 15:32:30 INFO     UCB-ExPLORe done for 4 actions!\n",
      "2024-04-09 15:32:32 INFO     BC done for 4 actions!\n",
      "2024-04-09 15:33:10 INFO     Naive-TS done for 5 actions!\n",
      "2024-04-09 15:33:14 INFO     Oracle-TS done for 5 actions!\n",
      "2024-04-09 15:34:46 INFO     ExPerior done for 5 actions!\n",
      "2024-04-09 15:34:48 INFO     Naive-UCB done for 5 actions!\n",
      "2024-04-09 15:34:50 INFO     UCB-ExPLORe done for 5 actions!\n",
      "2024-04-09 15:34:51 INFO     BC done for 5 actions!\n",
      "2024-04-09 15:35:26 INFO     Naive-TS done for 6 actions!\n",
      "2024-04-09 15:35:31 INFO     Oracle-TS done for 6 actions!\n",
      "2024-04-09 15:37:01 INFO     ExPerior done for 6 actions!\n",
      "2024-04-09 15:37:03 INFO     Naive-UCB done for 6 actions!\n",
      "2024-04-09 15:37:04 INFO     UCB-ExPLORe done for 6 actions!\n",
      "2024-04-09 15:37:06 INFO     BC done for 6 actions!\n",
      "2024-04-09 15:37:39 INFO     Naive-TS done for 7 actions!\n",
      "2024-04-09 15:37:44 INFO     Oracle-TS done for 7 actions!\n",
      "2024-04-09 15:39:16 INFO     ExPerior done for 7 actions!\n",
      "2024-04-09 15:39:18 INFO     Naive-UCB done for 7 actions!\n",
      "2024-04-09 15:39:20 INFO     UCB-ExPLORe done for 7 actions!\n",
      "2024-04-09 15:39:21 INFO     BC done for 7 actions!\n",
      "2024-04-09 15:39:50 INFO     Naive-TS done for 8 actions!\n",
      "2024-04-09 15:39:55 INFO     Oracle-TS done for 8 actions!\n",
      "2024-04-09 15:41:36 INFO     ExPerior done for 8 actions!\n",
      "2024-04-09 15:41:38 INFO     Naive-UCB done for 8 actions!\n",
      "2024-04-09 15:41:39 INFO     UCB-ExPLORe done for 8 actions!\n",
      "2024-04-09 15:41:41 INFO     BC done for 8 actions!\n",
      "2024-04-09 15:42:11 INFO     Naive-TS done for 9 actions!\n",
      "2024-04-09 15:42:17 INFO     Oracle-TS done for 9 actions!\n",
      "2024-04-09 15:43:53 INFO     ExPerior done for 9 actions!\n",
      "2024-04-09 15:43:55 INFO     Naive-UCB done for 9 actions!\n",
      "2024-04-09 15:43:56 INFO     UCB-ExPLORe done for 9 actions!\n",
      "2024-04-09 15:43:58 INFO     BC done for 9 actions!\n",
      "2024-04-09 15:44:26 INFO     Naive-TS done for 10 actions!\n",
      "2024-04-09 15:44:32 INFO     Oracle-TS done for 10 actions!\n",
      "2024-04-09 15:46:10 INFO     ExPerior done for 10 actions!\n",
      "2024-04-09 15:46:12 INFO     Naive-UCB done for 10 actions!\n",
      "2024-04-09 15:46:13 INFO     UCB-ExPLORe done for 10 actions!\n",
      "2024-04-09 15:46:15 INFO     BC done for 10 actions!\n"
     ]
    }
   ],
   "source": [
    "NUM_ACTIONS_LIST = range(2, 11)\n",
    "\n",
    "naive_ts = []\n",
    "naive_ucb = []\n",
    "ucb_explore = []\n",
    "oracle_ts = []\n",
    "bc = []\n",
    "experior = []\n",
    "expert_entropies_list = []\n",
    "\n",
    "for NUM_ACTIONS in NUM_ACTIONS_LIST:\n",
    "    rng = PRNGSequence(SETTING_SEED)\n",
    "\n",
    "    alpha_betas = jax.random.beta(next(rng), 1.0, 1.0, (NUM_PRIORS, NUM_ACTIONS, 2)) * 4\n",
    "\n",
    "    action_space = spaces.Discrete(NUM_ACTIONS)\n",
    "\n",
    "    def prior_function(key, i: Optional[int] = 0):\n",
    "        return jax.random.beta(key, alpha_betas[i, :, 0], alpha_betas[i, :, 1])\n",
    "\n",
    "    prior_function = jax.tree_util.Partial(prior_function)\n",
    "    reward_dist_fn = jax.tree_util.Partial(\n",
    "        lambda key, means, _, action: jax.random.bernoulli(key, means[action]).astype(\n",
    "            jnp.float32\n",
    "        )\n",
    "    )\n",
    "    best_action_value_fn = jax.tree_util.Partial(\n",
    "        lambda means, _: (means.argmax(), means.max())\n",
    "    )\n",
    "    reward_mean_fn = jax.tree_util.Partial(lambda means, _, action: means[action])\n",
    "    mutli_armed_bandit = BayesStochasticBandit(\n",
    "        action_space,\n",
    "        prior_function,\n",
    "        reward_dist_fn,\n",
    "        reward_mean_fn,\n",
    "        best_action_value_fn,\n",
    "    )\n",
    "\n",
    "    # generate the expert trajectories\n",
    "    expert_trajectories = jax.vmap(\n",
    "        generate_optimal_trajectories, in_axes=(None, None, None, None, 0)\n",
    "    )(\n",
    "        next(rng),\n",
    "        mutli_armed_bandit,\n",
    "        EXPERT_N_TRAJECTORY,\n",
    "        1,\n",
    "        jnp.arange(alpha_betas.shape[0]),\n",
    "    )  # shape: (n_priors, n_steps, n_envs, ...)\n",
    "\n",
    "    expert_fractions = jax.vmap(\n",
    "        lambda a: jnp.bincount(a.reshape(-1), length=NUM_ACTIONS) / EXPERT_N_TRAJECTORY\n",
    "    )(expert_trajectories.action)\n",
    "\n",
    "    # calculate the entropy of each expert trajectory\n",
    "    epsilon = 1e-8\n",
    "    entropy_fn = lambda exp_traj: -jax.vmap(lambda p: p * jnp.log2(p + epsilon))(\n",
    "        jnp.bincount(exp_traj.action.flatten(), length=NUM_ACTIONS)\n",
    "        / EXPERT_N_TRAJECTORY\n",
    "    ).sum()\n",
    "    expert_entropies = jax.vmap(entropy_fn)(expert_trajectories)\n",
    "\n",
    "    expert_entropies_list.append(expert_entropies)\n",
    "\n",
    "    # train the naive thompson sampling\n",
    "    feature_fn = jax.tree_util.Partial(\n",
    "        lambda obs, action: jax.nn.one_hot(action, NUM_ACTIONS)\n",
    "    )\n",
    "    reward_model = LinearDiscreteRewardModel(\n",
    "        n_actions=NUM_ACTIONS,\n",
    "        params_dim=NUM_ACTIONS,\n",
    "        feature_fn=feature_fn,\n",
    "        dist=\"bernoulli\",\n",
    "    )\n",
    "\n",
    "    ts_train = make_thompson_sampling(\n",
    "        env=mutli_armed_bandit,\n",
    "        reward_model=reward_model,\n",
    "        num_envs=NUM_ENVS,\n",
    "        total_steps=NUM_STEPS,\n",
    "        langevin_batch_size=ORACLE_TS_SGLD_BSIZE,\n",
    "        langevin_updates_per_step=ORACLE_TS_UPDATES_PER_STEP,\n",
    "    )\n",
    "\n",
    "    state, no_prior_metrics = jax.vmap(jax.jit(ts_train), in_axes=(None, None, 0))(\n",
    "        jax.random.PRNGKey(ENV_SEED),\n",
    "        ORACLE_TS_SGLD_LR,\n",
    "        jnp.arange(alpha_betas.shape[0]),\n",
    "    )  # shape: (n_priors, n_steps, n_envs, ...)\n",
    "\n",
    "    naive_ts.append(\n",
    "        (no_prior_metrics[\"optimal_value\"] - no_prior_metrics[\"reward_mean\"])\n",
    "        .mean(axis=-1)\n",
    "        .cumsum(axis=-1)\n",
    "    )\n",
    "    logging.info(f\"Naive-TS done for {NUM_ACTIONS} actions!\")\n",
    "\n",
    "    # train the oracle thompson sampling\n",
    "    prior_alpha_betas = lambda i: alpha_betas[i, :, :]\n",
    "    true_prior_ts_train = make_bernoulli_thompson_sampling(\n",
    "        env=mutli_armed_bandit,\n",
    "        num_envs=NUM_ENVS,\n",
    "        total_steps=NUM_STEPS,\n",
    "        prior_alpha_betas=prior_alpha_betas,\n",
    "    )\n",
    "\n",
    "    state, true_prior_metrics = jax.vmap(\n",
    "        jax.jit(true_prior_ts_train), in_axes=(None, 0)\n",
    "    )(jax.random.PRNGKey(ENV_SEED), jnp.arange(alpha_betas.shape[0]))\n",
    "\n",
    "    oracle_ts.append(\n",
    "        (true_prior_metrics[\"optimal_value\"] - true_prior_metrics[\"reward_mean\"])\n",
    "        .mean(axis=-1)\n",
    "        .cumsum(axis=-1)\n",
    "    )\n",
    "    logging.info(f\"Oracle-TS done for {NUM_ACTIONS} actions!\")\n",
    "\n",
    "    # train the max entropy thompson sampling (ExPerior)\n",
    "    reward_model = LinearDiscreteRewardModel(\n",
    "        n_actions=NUM_ACTIONS,\n",
    "        params_dim=NUM_ACTIONS,\n",
    "        feature_fn=feature_fn,\n",
    "        dist=\"bernoulli\",\n",
    "    )\n",
    "    max_ent_ts_train = make_max_ent_thompson_sampling(\n",
    "        env=mutli_armed_bandit,\n",
    "        reward_model=reward_model,\n",
    "        num_envs=NUM_ENVS,\n",
    "        total_steps=NUM_STEPS,\n",
    "        langevin_batch_size=EXPERIOR_SGLD_BSIZE,\n",
    "        langevin_updates_per_step=EXPERIOR_UPDATES_PER_STEP,\n",
    "        max_ent_prior_n_samples=MAX_ENT_SAMPLES,\n",
    "        max_ent_steps=MAX_ENT_STEPS,\n",
    "    )\n",
    "    state, max_ent_state, max_ent_metrics = jax.vmap(\n",
    "        jax.jit(max_ent_ts_train),\n",
    "        in_axes=(None, None, 0, None, None, None, None, 0),\n",
    "    )(\n",
    "        jax.random.PRNGKey(MAX_ENT_SEED),\n",
    "        jax.random.PRNGKey(ENV_SEED),\n",
    "        expert_trajectories,\n",
    "        EXPERIOR_LAMBDA,\n",
    "        MAX_ENT_LR,\n",
    "        EXPERIOR_BETA,\n",
    "        EXPERIOR_SGLD_LR,\n",
    "        jnp.arange(alpha_betas.shape[0]),\n",
    "    )\n",
    "    experior.append(\n",
    "        (max_ent_metrics[\"optimal_value\"] - max_ent_metrics[\"reward_mean\"])\n",
    "        .mean(axis=-1)\n",
    "        .cumsum(axis=-1)\n",
    "    )\n",
    "    logging.info(f\"ExPerior done for {NUM_ACTIONS} actions!\")\n",
    "\n",
    "    # naive ucb\n",
    "    ucb_train = make_multi_armed_ucb(\n",
    "        env=mutli_armed_bandit, num_envs=NUM_ENVS, total_steps=NUM_STEPS\n",
    "    )\n",
    "    state, ucb_metrics = jax.vmap(jax.jit(ucb_train), in_axes=(None, None, 0))(\n",
    "        jax.random.PRNGKey(ENV_SEED), UCB_RHO, jnp.arange(alpha_betas.shape[0])\n",
    "    )\n",
    "    naive_ucb.append(\n",
    "        (ucb_metrics[\"optimal_value\"] - ucb_metrics[\"reward_mean\"])\n",
    "        .mean(axis=-1)\n",
    "        .cumsum(axis=-1)\n",
    "    )\n",
    "    logging.info(f\"Naive-UCB done for {NUM_ACTIONS} actions!\")\n",
    "\n",
    "    # explore ucb\n",
    "    ucb_train = make_multi_armed_explore_ucb(\n",
    "        env=mutli_armed_bandit, num_envs=NUM_ENVS, total_steps=NUM_STEPS\n",
    "    )\n",
    "    state, explore_ucb_metrics = jax.vmap(\n",
    "        jax.jit(ucb_train), in_axes=(None, 0, None, None, 0)\n",
    "    )(\n",
    "        jax.random.PRNGKey(ENV_SEED),\n",
    "        expert_fractions,\n",
    "        UCB_RHO,\n",
    "        UCB_BURN_IN,\n",
    "        jnp.arange(alpha_betas.shape[0]),\n",
    "    )\n",
    "\n",
    "    ucb_explore.append(\n",
    "        (explore_ucb_metrics[\"optimal_value\"] - explore_ucb_metrics[\"reward_mean\"])\n",
    "        .mean(axis=-1)\n",
    "        .cumsum(axis=-1)\n",
    "    )\n",
    "    logging.info(f\"UCB-ExPLORe done for {NUM_ACTIONS} actions!\")\n",
    "\n",
    "    # bc\n",
    "    bc_train = make_multi_armed_bc(\n",
    "        env=mutli_armed_bandit, num_envs=NUM_ENVS, total_steps=NUM_STEPS\n",
    "    )\n",
    "    state, bc_metrics = jax.vmap(jax.jit(bc_train), in_axes=(None, 0, 0))(\n",
    "        jax.random.PRNGKey(ENV_SEED), expert_fractions, jnp.arange(alpha_betas.shape[0])\n",
    "    )\n",
    "\n",
    "    bc.append(\n",
    "        (bc_metrics[\"optimal_value\"] - bc_metrics[\"reward_mean\"])\n",
    "        .mean(axis=-1)\n",
    "        .cumsum(axis=-1)\n",
    "    )\n",
    "    logging.info(f\"BC done for {NUM_ACTIONS} actions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.savez(\n",
    "    \"../output/bandit/regret_results_temp.npz\",\n",
    "    naive_ts=naive_ts,\n",
    "    naive_ucb=naive_ucb,\n",
    "    ucb_explore=ucb_explore,\n",
    "    oracle_ts=oracle_ts,\n",
    "    bc=bc,\n",
    "    experior=experior,\n",
    "    expert_entropies_list=expert_entropies_list,\n",
    "    num_actions_list=NUM_ACTIONS_LIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results\n",
    "results = jnp.load(\"../output/bandit/regret_results.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from experior.utils import (\n",
    "    latexify,\n",
    "    FIG_WIDTH,\n",
    "    GOLDEN_RATIO,\n",
    "    FONT_SIZE,\n",
    "    LEGEND_SIZE,\n",
    "    LIGHT_COLORS,\n",
    ")\n",
    "\n",
    "mpl.use(\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison to baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix actions to 10\n",
    "action_ind = -1\n",
    "low_entropies = results[\"expert_entropies_list\"][action_ind] < 0.8\n",
    "mid_entropies = (results[\"expert_entropies_list\"][action_ind] >= 0.8) & (\n",
    "    results[\"expert_entropies_list\"][action_ind] <= 1.6\n",
    ")\n",
    "high_entropies = results[\"expert_entropies_list\"][action_ind] > 1.6\n",
    "# plot the mean bayes regret for each method under different entropy levels - use bar plots\n",
    "latexify(\n",
    "    FIG_WIDTH,\n",
    "    FIG_WIDTH * GOLDEN_RATIO * 0.3,\n",
    "    font_size=FONT_SIZE,\n",
    "    legend_size=LEGEND_SIZE,\n",
    "    labelsize=LEGEND_SIZE,\n",
    ")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(FIG_WIDTH, FIG_WIDTH * GOLDEN_RATIO * 0.3))\n",
    "method_names = [\n",
    "    r\"Oracle-TS\",\n",
    "    r\"ExPerior ({Ours})\",\n",
    "    r\"Naïve-TS\",\n",
    "    r\"Naïve-UCB\",\n",
    "    r\"UCB-ExPLORe\",\n",
    "    r\"BC\",\n",
    "]\n",
    "keys = [\n",
    "    \"oracle_ts\",\n",
    "    \"experior\",\n",
    "    \"naive_ts\",\n",
    "    \"naive_ucb\",\n",
    "    \"ucb_explore\",\n",
    "    \"bc\",\n",
    "]\n",
    "method_regrets = [results[key][action_ind] for key in keys]\n",
    "ax.set_ylabel(r\"Bayesian Regret\")\n",
    "width = 0.2\n",
    "x_values = [\n",
    "    j for i in range(len(method_names)) for j in [i - 1.1 * width, i, i + 1.1 * width]\n",
    "]\n",
    "colors = [LIGHT_COLORS[\"blue\"], LIGHT_COLORS[\"green\"], LIGHT_COLORS[\"red\"]]\n",
    "hatches = [\"\\\\\\\\\\\\\\\\\\\\\", \"---\", \"/////\"]\n",
    "ax.set_xticks(\n",
    "    x_values,\n",
    "    [j for i in range(len(method_names)) for j in [\"\", method_names[i], \"\"]],\n",
    "    ha=\"center\",\n",
    "    va=\"top\",\n",
    ")\n",
    "means = [\n",
    "    j\n",
    "    for i in range(len(method_names))\n",
    "    for j in [\n",
    "        method_regrets[i][low_entropies].mean(),\n",
    "        method_regrets[i][mid_entropies].mean(),\n",
    "        method_regrets[i][high_entropies].mean(),\n",
    "    ]\n",
    "]\n",
    "ax.set_yticks(range(0, 151, 50))\n",
    "ax.bar(\n",
    "    x_values,\n",
    "    means,\n",
    "    width=width,\n",
    "    color=colors * len(method_names),\n",
    "    hatch=hatches * len(method_names),\n",
    ")\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "labels = [r\"Low Entropy\", r\"Mid Entropy\", r\"High Entropy\"]\n",
    "handles = [\n",
    "    mpatches.Patch(facecolor=colors[i], label=labels[i], hatch=hatches[i])\n",
    "    for i in range(len(labels))\n",
    "]\n",
    "ax.legend(handles=handles, ncol=1)\n",
    "\n",
    "plt.subplots_adjust(left=0.07, right=0.95, bottom=0.24)\n",
    "\n",
    "fig.savefig(\"../output/bandit/baselines.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical regret analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 16:00:20 INFO     Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "2024-04-09 16:00:20 INFO     Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "/tmp/ipykernel_1483245/4256593893.py:67: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
      "  axis.scatter(\n"
     ]
    }
   ],
   "source": [
    "latexify(\n",
    "    0.75 * FIG_WIDTH,\n",
    "    FIG_WIDTH * GOLDEN_RATIO / 3,\n",
    "    font_size=7,\n",
    "    legend_size=LEGEND_SIZE,\n",
    "    labelsize=LEGEND_SIZE,\n",
    ")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(0.75 * FIG_WIDTH, FIG_WIDTH * GOLDEN_RATIO / 3))\n",
    "marker_size = 7\n",
    "\n",
    "bases = [\n",
    "    results[\"naive_ts\"],\n",
    "    results[\"experior\"],\n",
    "    results[\"oracle_ts\"],\n",
    "]\n",
    "names = [r\"Naïve-TS\", r\"ExPerior\", r\"Oracle-TS\"]\n",
    "color_list = [\"red\", \"blue\", \"green\", \"black\"]\n",
    "fill_colors = [\"red\", \"blue\", \"green\", \"black\"]\n",
    "markers = [\"v\", \"^\", \"x\", \"o\"]\n",
    "linestyles = [\"-.\", \"--\", \"-\", \":\"]\n",
    "linewidth = 1\n",
    "\n",
    "NUM_STEPS = results[\"oracle_ts\"][0].shape[1]\n",
    "\n",
    "# regret v.s. actions\n",
    "for i in range(len(bases)):\n",
    "    axis = axes[0]\n",
    "    means = jnp.array(\n",
    "        [bases[i][j][:, -1].mean() for j in range(len(results[\"num_actions_list\"]))]\n",
    "    )\n",
    "    stds = jnp.array(\n",
    "        [bases[i][j][:, -1].std() for j in range(len(results[\"num_actions_list\"]))]\n",
    "    )\n",
    "    axis.plot(\n",
    "        results[\"num_actions_list\"],\n",
    "        means,\n",
    "        label=names[i],\n",
    "        linewidth=linewidth,\n",
    "        linestyle=linestyles[i],\n",
    "        c=LIGHT_COLORS[color_list[i]],\n",
    "    )\n",
    "    if i == -1:\n",
    "        axis.errorbar(\n",
    "            results[\"num_actions_list\"],\n",
    "            means,\n",
    "            yerr=stds,\n",
    "            fmt=\"none\",\n",
    "            c=LIGHT_COLORS[color_list[i]],\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "    else:\n",
    "        axis.fill_between(\n",
    "            results[\"num_actions_list\"],\n",
    "            means - stds,\n",
    "            means + stds,\n",
    "            color=LIGHT_COLORS[fill_colors[i]],\n",
    "            alpha=0.5,\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "    axis.set_xticks(results[\"num_actions_list\"])\n",
    "\n",
    "axis.set_ylabel(r\"Bayesian Regret\")\n",
    "axis.set_xlabel(r\"Number of Arms, $K$\")\n",
    "# regret v.s. entropy for actions 10\n",
    "for i in range(len(bases)):\n",
    "    axis = axes[1]\n",
    "    axis.scatter(\n",
    "        results[\"expert_entropies_list\"][action_ind],\n",
    "        bases[i][action_ind][:, -1],\n",
    "        label=names[i],\n",
    "        s=marker_size,\n",
    "        marker=markers[i],\n",
    "        c=LIGHT_COLORS[color_list[i]],\n",
    "        alpha=0.6 if i == 1 else 1,\n",
    "    )\n",
    "axis.set_xlabel(r\"Entropy of Optimal Action\")\n",
    "\n",
    "\n",
    "# regret v.s. horizon for actions 10\n",
    "for i in range(len(bases)):\n",
    "    axis = axes[2]\n",
    "    means = bases[i][action_ind].mean(axis=0)\n",
    "    stds = bases[i][action_ind].std(axis=0)\n",
    "    axis.plot(\n",
    "        jnp.arange(1, NUM_STEPS + 1),\n",
    "        means,\n",
    "        label=names[i],\n",
    "        linestyle=linestyles[i],\n",
    "        linewidth=linewidth,\n",
    "        c=LIGHT_COLORS[color_list[i]],\n",
    "    )\n",
    "    if i == -1:\n",
    "        axis.errorbar(\n",
    "            jnp.arange(1, NUM_STEPS + 1)[::200],\n",
    "            means[::200],\n",
    "            yerr=stds[::200],\n",
    "            fmt=\"none\",\n",
    "            c=LIGHT_COLORS[color_list[i]],\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "    else:\n",
    "        axis.fill_between(\n",
    "            jnp.arange(1, NUM_STEPS + 1),\n",
    "            means - stds,\n",
    "            means + stds,\n",
    "            color=LIGHT_COLORS[fill_colors[i]],\n",
    "            alpha=0.5,\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "axis.set_xlabel(r\"Episodes, $T$\")\n",
    "hs0 = axes[1].get_legend_handles_labels()[0]\n",
    "hs1 = axes[0].get_legend_handles_labels()[0]\n",
    "\n",
    "handles = [(hs0[i], hs1[i]) for i in range(len(hs0))]\n",
    "fig.legend(handles, names, loc=\"upper center\", ncol=3)\n",
    "plt.subplots_adjust(left=0.07, bottom=0.25, right=0.97, top=0.83, wspace=0.2)\n",
    "fig.savefig(\"../output/bandit/emp_regret.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequentist regret bound v.s. entropy of optimal actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(px):\n",
    "    return -1 * jnp.dot(px, jnp.log(px) / jnp.log(2))\n",
    "\n",
    "\n",
    "def regret(px, T):\n",
    "    sum = 0\n",
    "    for i in range(len(px)):\n",
    "        for j in range(len(px)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            sum += jnp.sqrt(\n",
    "                (px[i] / (px[i] + px[j])) * (1 - px[i] / (px[i] + px[j]))\n",
    "            ) * (jnp.sqrt(px[i]) + jnp.sqrt(px[j]))\n",
    "    return sum * 2 * jnp.sqrt(T * jnp.log(T))\n",
    "\n",
    "\n",
    "def sample_simplex(key, n):\n",
    "    \"\"\"\n",
    "    Sample a probability vector of length n from the probability simplex.\n",
    "    \"\"\"\n",
    "    # Sample n points from a uniform distribution\n",
    "    key, rng1, rng2 = jax.random.split(key, 3)\n",
    "    random_points1 = jax.random.beta(rng1, 1, 10, shape=(n,))\n",
    "    random_points2 = jax.random.beta(rng2, 10, 1, shape=(n,))\n",
    "\n",
    "    p = jax.random.choice(key, 2, shape=(n,))\n",
    "    random_points = p * random_points1 + (1 - p) * random_points2\n",
    "\n",
    "    # Normalize to ensure the sum is 1\n",
    "    probability_vector = random_points / jnp.sum(random_points)\n",
    "    return probability_vector\n",
    "\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "K = 2\n",
    "n_samples = 500\n",
    "rng = jax.random.PRNGKey(42)\n",
    "for i in range(n_samples):\n",
    "    rng, key = jax.random.split(rng)\n",
    "    p = sample_simplex(key, K)\n",
    "    x.append(entropy(p))\n",
    "    y.append(regret(p, T=100))\n",
    "\n",
    "latexify(\n",
    "    0.25 * FIG_WIDTH,\n",
    "    FIG_WIDTH * GOLDEN_RATIO / 3,\n",
    "    font_size=FONT_SIZE,\n",
    "    legend_size=LEGEND_SIZE,\n",
    "    labelsize=LEGEND_SIZE,\n",
    ")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(0.25 * FIG_WIDTH, FIG_WIDTH * GOLDEN_RATIO / 3))\n",
    "\n",
    "ax.scatter(x, y, s=1, c=LIGHT_COLORS[\"black\"])\n",
    "ax.set_xlabel(r\"Entropy of Optimal Action\")\n",
    "plt.subplots_adjust(left=0.09, bottom=0.25, right=0.97, top=0.83, wspace=0.2)\n",
    "fig.savefig(\"../output/bandit/freq_regret.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "jax_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
