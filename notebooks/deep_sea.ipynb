{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from experior.utils import moving_average\n",
    "from experior.rl_agents import make_boot_dqn_train\n",
    "from experior.sampling import langevin_sampling\n",
    "from experior.envs import DeepSea\n",
    "from experior.experts import generate_optimal_trajectories\n",
    "from experior.prior_trainers import make_max_ent_log_pdf, make_max_ent_prior_train\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 2000\n",
    "NUM_ENVS = 30\n",
    "HORIZON = 30\n",
    "ENV_SEED = 7192\n",
    "METHOD_SEED = 42\n",
    "PRIOR_SEED = 4748\n",
    "\n",
    "\n",
    "def get_goal_col_dist(i):\n",
    "    if i == 0:\n",
    "        return lambda key, size: size - 1\n",
    "    else:\n",
    "        if i == 1:\n",
    "            min_value = 0.75\n",
    "        elif i == 2:\n",
    "            min_value = 0.5\n",
    "        elif i == 3:\n",
    "            min_value = 0.0\n",
    "        else:\n",
    "            raise ValueError(\"Invalid goal column distribution\")\n",
    "        return lambda key, size: jax.random.randint(\n",
    "            key, shape=(), minval=int(min_value * size), maxval=size\n",
    "        )\n",
    "\n",
    "\n",
    "goal_col_dists = [jax.tree_util.Partial(get_goal_col_dist(i)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:31:31 INFO     Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA\n",
      "2024-04-09 18:31:31 INFO     Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n"
     ]
    }
   ],
   "source": [
    "envs = [\n",
    "    DeepSea(size=HORIZON, goal_column_dist=goal_col_dist)\n",
    "    for goal_col_dist in goal_col_dists\n",
    "]\n",
    "EXPERT_TRAJ_SEED = 2874\n",
    "EXPERT_N_TRAJ = 1000\n",
    "expert_trajectories_list = [\n",
    "    generate_optimal_trajectories(\n",
    "        jax.random.PRNGKey(EXPERT_TRAJ_SEED), env, EXPERT_N_TRAJ, HORIZON\n",
    "    )\n",
    "    for env in envs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Boot-DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    activation: nn.activation = nn.relu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.Dense(feat)(x)\n",
    "            x = self.activation(x)\n",
    "        return nn.Dense(self.features[-1])(x)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    n_actions: int\n",
    "    n_hidden: int\n",
    "    n_features: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray):\n",
    "        features = [self.n_hidden, self.n_features, self.n_actions]\n",
    "        return MLP(features)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default parameters\n",
    "\n",
    "DQN_BUFFER_SIZE = 10000\n",
    "Q_NETWORK_CONFIG = {\"n_hidden\": 50, \"n_features\": 50}\n",
    "TARGET_FREQ = 4\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_FREQ = 1\n",
    "NUM_ENSEMBLES = 25\n",
    "EPSILON_FN = lambda _: 0.0\n",
    "LEARNING_STARTS = 128\n",
    "OPTIMIZER = optax.adam(learning_rate=1e-3)\n",
    "MASK_PROB = 1.0\n",
    "NOISE_SCALE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:31:36 INFO     Starting Naive Boot-DQN for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f37d90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:45:21 INFO     Starting Naive Boot-DQN for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f37d00>\n",
      "2024-04-09 18:59:04 INFO     Starting Naive Boot-DQN for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f34220>\n",
      "2024-04-09 19:12:47 INFO     Starting Naive Boot-DQN for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f379d0>\n"
     ]
    }
   ],
   "source": [
    "naive_boot_dqn_outputs = []\n",
    "\n",
    "for env in envs:\n",
    "    logging.info(f\"Starting Naive Boot-DQN for {env}\")\n",
    "    main_q_network = QNetwork(env.num_actions, **Q_NETWORK_CONFIG)\n",
    "    q_network = main_q_network\n",
    "\n",
    "    def q_init_fn(key, inputs):\n",
    "        return q_network.init(key, inputs)\n",
    "\n",
    "    boot_dqn_train = make_boot_dqn_train(\n",
    "        env,\n",
    "        q_network,\n",
    "        DQN_BUFFER_SIZE,\n",
    "        BATCH_SIZE,\n",
    "        EPISODES * HORIZON,\n",
    "        LEARNING_STARTS,\n",
    "        NUM_ENSEMBLES,\n",
    "        OPTIMIZER,\n",
    "        jax.tree_util.Partial(q_init_fn),\n",
    "        EPSILON_FN,\n",
    "    )\n",
    "\n",
    "    boot_dqn_train = jax.jit(boot_dqn_train)\n",
    "    env_params = jax.vmap(env.init_env, (0, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(ENV_SEED), NUM_ENVS), env.default_params\n",
    "    )\n",
    "\n",
    "    state, output = jax.vmap(boot_dqn_train, (0, 0, None, None, None, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(METHOD_SEED), NUM_ENVS),\n",
    "        env_params,\n",
    "        MASK_PROB,\n",
    "        NOISE_SCALE,\n",
    "        TRAIN_FREQ,\n",
    "        TARGET_FREQ,\n",
    "    )\n",
    "\n",
    "    naive_boot_dqn_outputs.append(\n",
    "        output[\"info\"][\"returned_episode_returns\"][:, ::HORIZON]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.savez(\"../output/deep_sea/naive_boot_dqn.npz\", jnp.array(naive_boot_dqn_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExPerior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for prior training\n",
    "PRIOR_EPOCHS = 5000\n",
    "PRIOR_BATCH_SIZE = 1500\n",
    "EXPERT_BETA = 5.0\n",
    "PRIOR_LR = 1e-2\n",
    "PRIOR_LAMBDA = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 19:26:30 INFO     Starting ExPerior for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f37d90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 19:41:00 INFO     Starting ExPerior for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f37d00>\n",
      "2024-04-09 19:55:18 INFO     Starting ExPerior for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f34220>\n",
      "2024-04-09 20:09:36 INFO     Starting ExPerior for <experior.envs.deep_sea.DeepSea object at 0x7f21c6f379d0>\n"
     ]
    }
   ],
   "source": [
    "experior_outputs = []\n",
    "j = 0\n",
    "for env, expert_trajectories in zip(envs, expert_trajectories_list):\n",
    "    logging.info(f\"Starting ExPerior for {env}\")\n",
    "\n",
    "    # the only hyperparameter we tuned\n",
    "    if j == 2:\n",
    "        PRIOR_REG = 1\n",
    "    elif j == 3:\n",
    "        PRIOR_REG = 10\n",
    "    else:\n",
    "        PRIOR_REG = 0.1\n",
    "\n",
    "    prior_q_network = QNetwork(env.num_actions, **Q_NETWORK_CONFIG)\n",
    "    max_ent_prior_train = make_max_ent_prior_train(\n",
    "        prior_q_network, PRIOR_EPOCHS, PRIOR_BATCH_SIZE\n",
    "    )\n",
    "    prior_rng = jax.random.PRNGKey(PRIOR_SEED)\n",
    "    prior_rng, rng_ = jax.random.split(prior_rng)\n",
    "    prior_state, prior_loss = jax.jit(max_ent_prior_train)(\n",
    "        rng_,\n",
    "        expert_trajectories,\n",
    "        EXPERT_BETA,\n",
    "        PRIOR_LR,\n",
    "        PRIOR_LAMBDA,\n",
    "        PRIOR_REG,\n",
    "    )\n",
    "\n",
    "    prior_log_pdf = make_max_ent_log_pdf(\n",
    "        prior_state, prior_q_network, expert_trajectories, EXPERT_BETA, PRIOR_REG\n",
    "    )\n",
    "\n",
    "    prior_rng, rng_ = jax.random.split(prior_rng)\n",
    "    init_params = prior_q_network.init(rng_, expert_trajectories.obs[0])\n",
    "    grad_opt = lambda g: jax.tree_util.tree_map(lambda x: jnp.clip(x, -50, 50), g)\n",
    "    prior_rng, rng_ = jax.random.split(prior_rng)\n",
    "    _, samples = langevin_sampling(\n",
    "        rng_, init_params, prior_log_pdf, 1e-2, 10000, grad_opt\n",
    "    )\n",
    "\n",
    "    num_samples = 1500\n",
    "    samples = jax.tree_util.tree_map(lambda p: p[-(2 * num_samples) :: 2], samples)\n",
    "\n",
    "    def q_init_fn(key, _):\n",
    "        ind = jax.random.randint(key, shape=(), minval=0, maxval=num_samples)\n",
    "        return jax.tree_util.tree_map(lambda x: x[ind], samples)\n",
    "\n",
    "    q_network = QNetwork(env.num_actions, **Q_NETWORK_CONFIG)\n",
    "\n",
    "    boot_max_ent_train = make_boot_dqn_train(\n",
    "        env,\n",
    "        q_network,\n",
    "        DQN_BUFFER_SIZE,\n",
    "        BATCH_SIZE,\n",
    "        EPISODES * HORIZON,\n",
    "        LEARNING_STARTS,\n",
    "        NUM_ENSEMBLES,\n",
    "        OPTIMIZER,\n",
    "        jax.tree_util.Partial(q_init_fn),\n",
    "        EPSILON_FN,\n",
    "    )\n",
    "\n",
    "    boot_max_ent_train = jax.jit(boot_max_ent_train)\n",
    "    env_params = jax.vmap(env.init_env, (0, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(ENV_SEED), NUM_ENVS), env.default_params\n",
    "    )\n",
    "    state, max_ent_output = jax.vmap(\n",
    "        boot_max_ent_train, (0, 0, None, None, None, None)\n",
    "    )(\n",
    "        jax.random.split(jax.random.PRNGKey(METHOD_SEED), NUM_ENVS),\n",
    "        env_params,\n",
    "        MASK_PROB,\n",
    "        NOISE_SCALE,\n",
    "        TRAIN_FREQ,\n",
    "        TARGET_FREQ,\n",
    "    )\n",
    "    experior_outputs.append(\n",
    "        max_ent_output[\"info\"][\"returned_episode_returns\"][:, ::HORIZON]\n",
    "    )\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.savez(\"../output/deep_sea/experior.npz\", jnp.array(experior_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "experior_outputs = jnp.load(\"../output/deep_sea/experior.npz\")[\"arr_0\"]\n",
    "naive_boot_dqn_outputs = jnp.load(\"../output/deep_sea/naive_boot_dqn.npz\")[\"arr_0\"]\n",
    "explore_outputs = jnp.load(\"../output/deep_sea/explore.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "from experior.utils import (\n",
    "    latexify,\n",
    "    FIG_WIDTH,\n",
    "    GOLDEN_RATIO,\n",
    "    FONT_SIZE,\n",
    "    LEGEND_SIZE,\n",
    "    LIGHT_COLORS,\n",
    ")\n",
    "\n",
    "mpl.use(\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rewards = 0.99\n",
    "min_rewards = 0.0\n",
    "\n",
    "latexify(\n",
    "    FIG_WIDTH,\n",
    "    FIG_WIDTH * GOLDEN_RATIO * 0.8,\n",
    "    font_size=FONT_SIZE,\n",
    "    legend_size=LEGEND_SIZE,\n",
    "    labelsize=LEGEND_SIZE,\n",
    ")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(FIG_WIDTH, FIG_WIDTH * GOLDEN_RATIO * 0.8))\n",
    "window = 5\n",
    "start = 1\n",
    "bases = [\n",
    "    [\n",
    "        moving_average(\n",
    "            naive_boot_dqn_outputs[i].mean(0)[start:],\n",
    "            window,\n",
    "        ),\n",
    "        moving_average(\n",
    "            experior_outputs[i].mean(0)[start:],\n",
    "            window,\n",
    "        ),\n",
    "        moving_average(explore_outputs[i].mean(0)[start:], window),\n",
    "    ]\n",
    "    for i in range(4)\n",
    "]\n",
    "stds = [\n",
    "    [\n",
    "        moving_average(\n",
    "            naive_boot_dqn_outputs[i].std(0)[start:],\n",
    "            window,\n",
    "        ),\n",
    "        moving_average(\n",
    "            experior_outputs[i].std(0)[start:],\n",
    "            window,\n",
    "        ),\n",
    "        moving_average(explore_outputs[i].std(0)[start:], window),\n",
    "    ]\n",
    "    for i in range(4)\n",
    "]\n",
    "names = [r\"Na√Øve Boot-DQN\", r\"ExPerior ({Ours})\", r\"ExPLORe\"]\n",
    "color_list = [\"red\", \"blue\", \"green\"]\n",
    "fill_colors = [\"red\", \"blue\", \"green\"]\n",
    "linestyles = [\"-\"] * 3\n",
    "\n",
    "for i in range(len(bases)):\n",
    "    x, y = i // 2, i % 2\n",
    "    axis = axes[x, y]\n",
    "    for j, base in enumerate(bases[i]):\n",
    "        ranges = [k + 1 for k in range(len(base))]\n",
    "        axis.plot(\n",
    "            ranges,\n",
    "            base,\n",
    "            label=names[j],\n",
    "            linewidth=1.5,\n",
    "            linestyle=linestyles[j],\n",
    "            c=LIGHT_COLORS[color_list[j]],\n",
    "        )\n",
    "        axis.fill_between(\n",
    "            ranges,\n",
    "            np.clip(base - stds[i][j], min_rewards, max_rewards),\n",
    "            np.clip(base + stds[i][j], min_rewards, max_rewards),\n",
    "            color=LIGHT_COLORS[fill_colors[j]],\n",
    "            alpha=0.3,\n",
    "            linewidth=1,\n",
    "        )\n",
    "    if x == 1:\n",
    "        axis.set_xlabel(r\"Episodes, $T$\")\n",
    "    if y == 0:\n",
    "        axis.set_ylabel(r\"Average Reward\")\n",
    "\n",
    "hs = axes[0, 0].get_legend_handles_labels()[0]\n",
    "\n",
    "fig.legend(hs, names, loc=\"upper center\", ncol=3)\n",
    "plt.subplots_adjust(left=0.1, right=0.97)\n",
    "\n",
    "fig.savefig(\"../output/deep_sea/deep_sea_results.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "jax_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
